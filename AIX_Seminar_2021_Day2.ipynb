{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIX_Seminar_2021_Day2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/AIX_Seminar_GPT-2_2021_Day1_2/blob/main/AIX_Seminar_2021_Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LEs17_wA4k"
      },
      "source": [
        "**メニュー「ランタイム→ランタイムのタイプを変更」**でハードウェアアクセラレータを**GPU**に変更して保存してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZWt9BJkZair",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13dbf7e0-670c-4615-cab2-652e1012d9b2"
      },
      "source": [
        "# GoogleDriveをマウントする\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#作業用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/'\n",
        "\n",
        "#青空文庫データ保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/aozora_data'\n",
        "\n",
        "# 学習済みモデル保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/bert_data'\n",
        "\n",
        "#作業用フォルダに移動する\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/AIX_seminner_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P8dPYhArF_W"
      },
      "source": [
        "**マイドライブ＞AIX_seminner_2021>aozora_data** 内に青空文庫からダウンロードしたzipファイルを入れてください．\n",
        "[青空文庫](https://www.aozora.gr.jp/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTSr6cSZ5ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94417e8d-f70c-4568-a924-1f3f8fcec0b8"
      },
      "source": [
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91\n",
        "\n",
        "# transformersのインストール\n",
        "!pip install transformers==4.4.2 tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, tqdm, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting transformers==4.4.2\n",
            "  Downloading transformers-4.4.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.49.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuBnZFikaM-1",
        "outputId": "c00f0b86-bdb2-41b3-c6f6-dfa6c69befca"
      },
      "source": [
        "#google drive用ダウンロードツールのインストール\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBdrbH740it_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8082fc4-9990-461e-86d4-3eb1ddde0ec5"
      },
      "source": [
        "# 作業フォルダに戻る\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/AIX_seminner_2021/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZn7cRlbZ-Eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624937b3-beb2-471a-c1e7-909094f6afe2"
      },
      "source": [
        "#aozora_dataファルダに移動\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/aozora_data'\n",
        "#フォルダ内の既存のtxtファイルをすべて削除\n",
        "!rm *.txt\n",
        "#フォルダ内のzipファイルを展開する\n",
        "!unzip '*.zip'\n",
        "#作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/AIX_seminner_2021/aozora_data\n",
            "Archive:  773_ruby_5968.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: kokoro.txt              \n",
            "\n",
            "Archive:  875_ruby_3249.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: chikyu_hakkyo_jiken.txt  \n",
            "\n",
            "Archive:  1465_ruby_16804.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: kanikosen.txt           \n",
            "\n",
            "Archive:  45621_ruby_20106.zip\n",
            "  inflating: ango_suji.txt           \n",
            "\n",
            "Archive:  2714_ruby_5772.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: angono_yakuwari.txt     \n",
            "\n",
            "Archive:  3238_ruby_9508.zip\n",
            "  inflating: ango_record_jiken.txt   \n",
            "\n",
            "Archive:  3527_ruby_18323.zip\n",
            "  inflating: ukabu_hikoto.txt        \n",
            "\n",
            "Archive:  3355_ruby_13869.zip\n",
            "  inflating: uchu_senpei.txt         \n",
            "\n",
            "Archive:  3354_ruby_12227.zip\n",
            "  inflating: uchuno_maigo.txt        \n",
            "\n",
            "Archive:  3356_ruby_13870.zip\n",
            "  inflating: eihondo_joriku.txt      \n",
            "\n",
            "Archive:  1237_ruby_7222.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: osorosiki_tsuya.txt     \n",
            "\n",
            "Archive:  1224_ruby_28092.zip\n",
            "  inflating: kaidan.txt              \n",
            "\n",
            "Archive:  1256_ruby_50014.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: kaiteitairiku.txt       \n",
            "\n",
            "Archive:  2659_ruby_23878.zip\n",
            "  inflating: kaiteitoshi.txt         \n",
            "\n",
            "Archive:  3049_ruby_17748.zip\n",
            "  inflating: kasei_tanken.txt        \n",
            "\n",
            "Archive:  1248_ruby_18617.zip\n",
            "  inflating: kakiirono_kamifusen.txt  \n",
            "\n",
            "Archive:  3521_ruby_18321.zip\n",
            "  inflating: kaminari.txt            \n",
            "\n",
            "Archive:  3519_ruby_20404.zip\n",
            "  inflating: kasokoku_fukei.txt      \n",
            "\n",
            "Archive:  3367_ruby_13362.zip\n",
            "  inflating: uchu_sentai.txt         \n",
            "\n",
            "Archive:  3368_ruby_25107.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: kaseiheidan.txt         \n",
            "\n",
            "Archive:  2638_ruby_23932.zip\n",
            "  inflating: kaisei_gan.txt          \n",
            "\n",
            "Archive:  3369_ruby_14605.zip\n",
            "  inflating: kayakusen.txt           \n",
            "\n",
            "Archive:  2713_ruby_28476.zip\n",
            "  inflating: kizoku_higan.txt        \n",
            "\n",
            "Archive:  3532_ruby_36922.zip\n",
            "  inflating: kan'okeno_hanayome.txt  \n",
            "\n",
            "Archive:  1243_ruby_18302.zip\n",
            "  inflating: kanchoza_jiken.txt      \n",
            "\n",
            "Archive:  3230_ruby_7211.zip\n",
            "  inflating: kibutsudojiken.txt      \n",
            "\n",
            "Archive:  1250_ruby_18618.zip\n",
            "  inflating: gimonno_kinkai.txt      \n",
            "\n",
            "27 archives were successfully processed.\n",
            "/content/drive/My Drive/AIX_seminner_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6_YZ4eWaKcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3480221a-033c-4738-94f9-7a1881a14b76"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "\n",
        "#ファインチューニング用のデータを作成する\n",
        "train_text_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "  #テキストを一文ずつ分割したデータを作成\n",
        "  for sentence in text[0:-1]:\n",
        "    if len(sentence):\n",
        "      train_text_list.append(sentence.strip().replace('\\n',''))\n",
        "\n",
        "#データをtrain.txtとして保存\n",
        "with open(\"train.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(train_text_list))\n",
        "print(\"Write\", len(train_text_list), \"lines.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write 1901 lines.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrs6yHdaQeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd80f9e-1d0b-4524-b580-1516d56e2f9c"
      },
      "source": [
        "# GPT-2ファインチューニング実行用ファイルのダウンロード\n",
        "!gdown https://drive.google.com/uc?id=1I9PmXXzLkqaILJLvlq3g6wtVfi8_Z2a1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I9PmXXzLkqaILJLvlq3g6wtVfi8_Z2a1\n",
            "To: /content/drive/My Drive/AIX_seminner_2021/run_clm.py\n",
            "\r  0% 0.00/18.7k [00:00<?, ?B/s]\r100% 18.7k/18.7k [00:00<00:00, 5.65MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxjod_AYaUH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d6b787-1509-45dc-f1e3-fcc495b361d9"
      },
      "source": [
        "%%time\n",
        "\n",
        "!rm -r ./output\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False \\\n",
        "    --block_size 512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './output': No such file or directory\n",
            "08/30/2021 15:29:49 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/30/2021 15:29:49 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Aug30_15-29-49_27c155066f18, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 2.00MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-66d7882a1b8b9a46 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-66d7882a1b8b9a46/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-66d7882a1b8b9a46/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-08-30 15:29:50,169 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp31h2cu8s\n",
            "Downloading: 100% 799/799 [00:00<00:00, 674kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-30 15:29:50,304 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|file_utils.py:1393] 2021-08-30 15:29:50,304 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:463] 2021-08-30 15:29:50,305 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:499] 2021-08-30 15:29:50,305 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-08-30 15:29:50,443 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpplu3e7rl\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 5.93MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-30 15:29:50,845 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-08-30 15:29:50,845 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-08-30 15:29:51,113 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpig123is9\n",
            "Downloading: 100% 153/153 [00:00<00:00, 147kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-30 15:29:51,249 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-08-30 15:29:51,249 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-08-30 15:29:51,385 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4vqxkuli\n",
            "Downloading: 100% 282/282 [00:00<00:00, 150kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-30 15:29:51,556 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|file_utils.py:1393] 2021-08-30 15:29:51,556 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-30 15:29:51,688 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-30 15:29:51,688 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-30 15:29:51,688 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-30 15:29:51,688 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-30 15:29:51,688 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-08-30 15:29:51,886 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvcxzxib9\n",
            "Downloading: 100% 1.37G/1.37G [00:40<00:00, 33.6MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-30 15:30:32,962 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|file_utils.py:1393] 2021-08-30 15:30:32,962 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1051] 2021-08-30 15:30:32,963 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1167] 2021-08-30 15:30:43,412 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-08-30 15:30:43,412 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 2/2 [00:00<00:00,  9.38ba/s]\n",
            "100% 2/2 [00:00<00:00,  8.61ba/s]\n",
            "100% 2/2 [00:00<00:00, 10.85ba/s]\n",
            "100% 2/2 [00:00<00:00, 10.40ba/s]\n",
            "[INFO|trainer.py:946] 2021-08-30 15:30:54,642 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-08-30 15:30:54,642 >>   Num examples = 69\n",
            "[INFO|trainer.py:948] 2021-08-30 15:30:54,642 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:949] 2021-08-30 15:30:54,642 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:950] 2021-08-30 15:30:54,642 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:951] 2021-08-30 15:30:54,642 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-08-30 15:30:54,642 >>   Total optimization steps = 207\n",
            "100% 207/207 [03:32<00:00,  1.03s/it][INFO|trainer.py:1129] 2021-08-30 15:34:27,591 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 212.9486, 'train_samples_per_second': 0.972, 'epoch': 3.0}\n",
            "100% 207/207 [03:32<00:00,  1.03s/it]\n",
            "[INFO|trainer.py:1558] 2021-08-30 15:34:27,996 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:314] 2021-08-30 15:34:28,002 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-30 15:34:34,543 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-30 15:34:34,549 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-30 15:34:34,554 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-30 15:34:34,564 >> Copy vocab file to output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-30 15:34:34,961 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,961 >>   epoch                      =      3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,961 >>   init_mem_cpu_alloc_delta   =      1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   init_mem_cpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   init_mem_gpu_alloc_delta   =   1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   init_mem_gpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   train_mem_cpu_alloc_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   train_mem_cpu_peaked_delta =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   train_mem_gpu_alloc_delta  =   3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   train_mem_gpu_peaked_delta =   2876MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,962 >>   train_runtime              = 212.9486\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,963 >>   train_samples              =       69\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:34,963 >>   train_samples_per_second   =    0.972\n",
            "08/30/2021 15:34:35 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1775] 2021-08-30 15:34:35,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1776] 2021-08-30 15:34:35,138 >>   Num examples = 69\n",
            "[INFO|trainer.py:1777] 2021-08-30 15:34:35,138 >>   Batch size = 1\n",
            "100% 69/69 [00:18<00:00,  3.76it/s]\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-30 15:34:53,749 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,749 >>   epoch                     =     3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,750 >>   eval_loss                 =  2.6542\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,750 >>   eval_mem_cpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,750 >>   eval_mem_cpu_peaked_delta =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,750 >>   eval_mem_gpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,750 >>   eval_mem_gpu_peaked_delta =   270MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,751 >>   eval_runtime              = 18.4054\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,751 >>   eval_samples              =      69\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,751 >>   eval_samples_per_second   =   3.749\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 15:34:53,751 >>   perplexity                = 14.2129\n",
            "CPU times: user 3.24 s, sys: 635 ms, total: 3.87 s\n",
            "Wall time: 5min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpscAmQ3sI3b"
      },
      "source": [
        "**GPT2のみを用いた生成**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pdygRzqTPDjB",
        "outputId": "d2e4df29-82d6-4825-d1ca-625707e76f80"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "# ファインチューニングしたモデルを用いる\n",
        "USE_FINETUNED_GPT2 = True\n",
        "# ファインチューニングしたモデルを用いない\n",
        "# USE_FINETUNED_GPT2 = False\n",
        "\n",
        "#候補文をいくつ表示するか\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "#モデルの準備\n",
        "if USE_FINETUNED_GPT2:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "else:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"retry\" == u:\n",
        "    next_sentence = log.pop()\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=OPTION_NUM)\n",
        "\n",
        "  sequence_list = []\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    sequence_list.append(sequence)\n",
        "  for i,sequence in enumerate(sequence_list):\n",
        "    print(\"[\", i,\"]\",sequence)\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_list[int(u)]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"retry\"と入力すると同じ文で再度文を生成します\n",
        "  # 7. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3ba349b4d766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#モデルの準備\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_FINETUNED_GPT2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rinna/japanese-gpt2-medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             return MODEL_FOR_CAUSAL_LM_MAPPING[type(config)].from_pretrained(\n\u001b[0;32m-> 1113\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m             )\n\u001b[1;32m   1115\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membd_pdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membd_pdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_ctx, config, scale)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0minner_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inner\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nx, n_ctx, config, scale, is_cross_attention)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_pdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nf, nx)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReW2XN6orDxa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "**ここからBERTの学習とそれを活用した推論**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvrB6gXYcfHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3aa4a45-a6f3-426e-9549-473844338305"
      },
      "source": [
        "#ライブラリのインストール\n",
        "!apt install git make curl xz-utils file\n",
        "!apt install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "make is already the newest version (4.1-9.1ubuntu1).\n",
            "make set to manually installed.\n",
            "xz-utils is already the newest version (5.2.2-1.3).\n",
            "xz-utils set to manually installed.\n",
            "curl is already the newest version (7.58.0-2ubuntu3.14).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 2s (146 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 148489 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-utils\n",
            "0 upgraded, 6 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 12.8 MB of archives.\n",
            "After this operation, 60.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 12.8 MB in 3s (3,771 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 148521 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../3-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../4-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../5-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.996.5\n",
            "  Downloading mecab_python3-0.996.5-cp37-cp37m-manylinux2010_x86_64.whl (17.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 71 kB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.1.1\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 199 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=a59f11fb7492396a0293c925b27896d0f311e359dc417e24370754178a892921\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikU7fNEDc-4g"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import random\n",
        "\n",
        "#aozora_dataフォルダ内のtxtファイルをつかってpairデータを作成する\n",
        "pair_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "    #1～3文と1～3文が対応するペアデータを作成\n",
        "    for i in range(len(text)-6):\n",
        "      m = random.randint(1,3)\n",
        "      n = random.randint(1,3)\n",
        "      pair_list.append(\"\".join(text[i:i+m])+\"\\t\"+\"\".join(text[i+m:i+m+n]))\n",
        "\n",
        "#ペアデータをpair.txtとして保存\n",
        "with open(\"pair.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(pair_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc2unyvApHhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc69de2-1a0e-4929-d095-e89488654462"
      },
      "source": [
        "#BERTの学習用ファイルのダウンロード\n",
        "!gdown https://drive.google.com/uc?id=1VOsR57Zoyy97lxDMGF1euTzNQJpyoZ7E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VOsR57Zoyy97lxDMGF1euTzNQJpyoZ7E\n",
            "To: /content/run_glue.py\n",
            "\r  0% 0.00/23.4k [00:00<?, ?B/s]\r100% 23.4k/23.4k [00:00<00:00, 46.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1H7AxOdCDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed8c4b1-edd4-49c2-e582-4d79dc78c3ad"
      },
      "source": [
        "write_lines = []\n",
        "uttrs = []\n",
        "\n",
        "filename = 'pair.txt'\n",
        "\n",
        "with open(filename) as f:\n",
        "    for l in f:\n",
        "        l = l.strip()\n",
        "        if \"\\t\" in l:\n",
        "            # 実際の応答ペアを正解とし，ラベルは1とする．\n",
        "            write_lines.append('1,\"' + l.replace(\"\\t\", \"\") + '\"' + \"\\n\")\n",
        "            # 不正解ペアの作成のため，発話を保存\n",
        "            uttrs.append(l.split(\"\\t\")[0])\n",
        "            uttrs.append(l.split(\"\\t\")[1])\n",
        "  \n",
        "# 正解ペアと同じ数だけ不正解ペアを作成\n",
        "for i in range(len(write_lines)):\n",
        "    # ランダムな応答ペアを不正解とし，ラベルは0とする．\n",
        "    write_lines.append('0,\"' + random.choice(uttrs) + random.choice(uttrs) + '\"' + \"\\n\")\n",
        "  \n",
        " # 正解ペアと不正解ペアが入ったリストをシャッフルする\n",
        "random.shuffle(write_lines)\n",
        "  \n",
        "index = 0\n",
        "with open(\"bert_data/dev.csv\", \"w\") as var_f:\n",
        "    var_f.write(\"label,sentence\\n\")\n",
        "    # 開発データとしてdev.tsvに200行を書き込む．\n",
        "    for l in write_lines[:200]:\n",
        "        var_f.write(l)\n",
        "        index += 1\n",
        "index = 0\n",
        "with open(\"bert_data/train.csv\", \"w\") as var_f:\n",
        "    var_f.write(\"label,sentence\\n\")\n",
        "    # 学習データとしてtrain.tsvにのこりを書き込む．\n",
        "    for l in write_lines[200:]:\n",
        "        var_f.write(l)\n",
        "        index += 1\n",
        "print(\"Write\", len(write_lines[200:]), \"lines to train.tsv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write 46256 lines to train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBubSjUadMCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda7661d-f842-4dad-e132-7c30b24d8e5a"
      },
      "source": [
        "# max_stepsの値を大きな値に設定することで，より多くのデータで学習できるが，より多くの時間が必要となる．\n",
        "# 時間がかかるので100に設定しているが，実際は全然足らない．10000以上には設定したい．\n",
        "!python ./run_glue.py --train_file ./bert_data/train.csv --validation_file ./bert_data/dev.csv  --overwrite_output_dir --overwrite_cache \\\n",
        "--model_name_or_path cl-tohoku/bert-base-japanese-whole-word-masking  --save_steps 100 --max_steps 100 \\\n",
        "--output_dir bert_output/ --do_train --do_eval --per_gpu_train_batch_size 16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_glue.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                   [--config_name CONFIG_NAME]\n",
            "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                   [--no_use_fast_tokenizer]\n",
            "                   [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "                   [--model_revision MODEL_REVISION]\n",
            "                   [--use_auth_token [USE_AUTH_TOKEN]] [--task_name TASK_NAME]\n",
            "                   [--max_seq_length MAX_SEQ_LENGTH]\n",
            "                   [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                   [--no_pad_to_max_length]\n",
            "                   [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "                   [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                   [--max_val_samples MAX_VAL_SAMPLES]\n",
            "                   [--max_test_samples MAX_TEST_SAMPLES]\n",
            "                   [--train_file TRAIN_FILE]\n",
            "                   [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]\n",
            "                   --output_dir OUTPUT_DIR\n",
            "                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                   [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                   [--do_predict [DO_PREDICT]]\n",
            "                   [--evaluation_strategy {no,steps,epoch}]\n",
            "                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                   [--learning_rate LEARNING_RATE]\n",
            "                   [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                   [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                   [--max_grad_norm MAX_GRAD_NORM]\n",
            "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                   [--max_steps MAX_STEPS]\n",
            "                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                   [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
            "                   [--logging_dir LOGGING_DIR]\n",
            "                   [--logging_strategy {no,steps,epoch}]\n",
            "                   [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                   [--logging_steps LOGGING_STEPS]\n",
            "                   [--save_strategy {no,steps,epoch}]\n",
            "                   [--save_steps SAVE_STEPS]\n",
            "                   [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda [NO_CUDA]]\n",
            "                   [--seed SEED] [--fp16 [FP16]]\n",
            "                   [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                   [--fp16_backend {auto,amp,apex}]\n",
            "                   [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                   [--local_rank LOCAL_RANK] [--tpu_num_cores TPU_NUM_CORES]\n",
            "                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug [DEBUG]]\n",
            "                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                   [--eval_steps EVAL_STEPS]\n",
            "                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                   [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                   [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
            "                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                   [--greater_is_better GREATER_IS_BETTER]\n",
            "                   [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                   [--sharded_ddp SHARDED_DDP] [--deepspeed DEEPSPEED]\n",
            "                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                   [--adafactor [ADAFACTOR]]\n",
            "                   [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                   [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                   [--no_dataloader_pin_memory]\n",
            "                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                   [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "run_glue.py: error: the following arguments are required: --model_name_or_path, --output_dir\n",
            "08/31/2021 02:42:00 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/31/2021 02:42:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=bert_output/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Aug31_02-42-00_3a764e586db9, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=100, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=bert_output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/wnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
            "[INFO|configuration_utils.py:463] 2021-08-31 02:42:01,933 >> loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "[INFO|configuration_utils.py:499] 2021-08-31 02:42:01,934 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:463] 2021-08-31 02:42:02,678 >> loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "[INFO|configuration_utils.py:499] 2021-08-31 02:42:02,678 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 02:42:06,481 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 02:42:06,481 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 02:42:06,481 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 02:42:06,481 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 02:42:06,481 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1051] 2021-08-31 02:42:07,268 >> loading weights file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb\n",
            "[WARNING|modeling_utils.py:1159] 2021-08-31 02:42:10,659 >> Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1170] 2021-08-31 02:42:10,659 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00,  3.79ba/s]\n",
            "100% 1/1 [00:00<00:00, 34.40ba/s]\n",
            "100% 1/1 [00:00<00:00, 12.64ba/s]\n",
            "08/31/2021 02:42:11 - INFO - __main__ -   Sample 114 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 114, 'input_ids': [2, 2401, 20368, 28825, 228, 27072, 28600, 28566, 1606, 582, 13268, 3560, 28658, 899, 11887, 2835, 492, 7589, 27820, 2480, 28589, 28807, 143, 18451, 28550, 2953, 27820, 2480, 28589, 28807, 1507, 936, 15815, 26844, 24049, 414, 17097, 10807, 8810, 275, 3152, 4562, 18868, 1629, 3513, 16398, 28550, 822, 8888, 28589, 8969, 28548, 12609, 228, 2142, 28825, 2835, 9576, 3560, 28890, 28511, 14341, 10196, 361, 2835, 27065, 28589, 143, 3, 27820, 2480, 28589, 28807, 24049, 3560, 28890, 28511, 14341, 10196, 361, 2835, 27065, 28589, 143, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/31/2021 02:42:11 - INFO - __main__ -   Sample 25 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 25, 'input_ids': [2, 2051, 19873, 13627, 28825, 24744, 636, 28550, 1507, 822, 1044, 828, 2747, 18688, 2142, 3940, 10695, 936, 27277, 24103, 5467, 10807, 24103, 24281, 19872, 11245, 5855, 28538, 8969, 28548, 4209, 143, 3, 2051, 936, 27277, 24103, 5467, 11245, 5855, 28538, 8969, 28548, 4209, 143, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/31/2021 02:42:11 - INFO - __main__ -   Sample 281 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 281, 'input_ids': [2, 18269, 19873, 26096, 5855, 725, 4337, 20195, 7589, 2142, 17366, 5550, 228, 1373, 2480, 15708, 24049, 28550, 1507, 822, 936, 6462, 20253, 27933, 143, 3, 20195, 24049, 28550, 1507, 822, 936, 6462, 20253, 27933, 143, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:369] 2021-08-31 02:42:15,445 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "[INFO|trainer.py:483] 2021-08-31 02:42:15,446 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
            "[INFO|trainer.py:483] 2021-08-31 02:42:15,446 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
            "[WARNING|training_args.py:607] 2021-08-31 02:42:15,661 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:607] 2021-08-31 02:42:15,666 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:946] 2021-08-31 02:42:15,666 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-08-31 02:42:15,666 >>   Num examples = 635\n",
            "[INFO|trainer.py:948] 2021-08-31 02:42:15,666 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:949] 2021-08-31 02:42:15,666 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:950] 2021-08-31 02:42:15,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:951] 2021-08-31 02:42:15,667 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-08-31 02:42:15,667 >>   Total optimization steps = 100\n",
            "[WARNING|training_args.py:607] 2021-08-31 02:42:15,678 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            " 26% 26/100 [00:19<00:54,  1.35it/s]Traceback (most recent call last):\n",
            "  File \"./run_glue.py\", line 527, in <module>\n",
            "    main()\n",
            "  File \"./run_glue.py\", line 459, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1083, in train\n",
            "    self.args.max_grad_norm,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\", line 43, in clip_grad_norm_\n",
            "    if total_norm.isnan() or total_norm.isinf():\n",
            "KeyboardInterrupt\n",
            " 26% 26/100 [00:20<00:57,  1.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrI5QftFgccn"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BertEvaluator:\n",
        "    def __init__(self):\n",
        "        # 事前学習済みのトークナイザとモデルをロード\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', do_lower_case=False)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
        "        \n",
        "        # Google Colabでファインチューニングしたモデルをロード\n",
        "        self.model.load_state_dict(torch.load(\"bert_output/pytorch_model.bin\", map_location=\"cpu\"))\n",
        "        self.model.to(device)\n",
        "\n",
        "    def evaluate(self, user_input, candidate):\n",
        "        with torch.no_grad():\n",
        "            # 発話のペアを特徴ベクトルに変換\n",
        "            tokenized = self.tokenizer([user_input+candidate], return_tensors=\"pt\")\n",
        "            input_ids = tokenized[\"input_ids\"].to(device)\n",
        "            token_type_ids = tokenized[\"token_type_ids\"].to(device)\n",
        "\n",
        "            # ファインチューニング済みのBERTを用いて特徴ベクトルから2文のスコアを計算\n",
        "            result = self.model.forward(input_ids, token_type_ids=token_type_ids)\n",
        "            # softmax関数によりスコアを正規化\n",
        "            result = F.softmax(result[0], dim=1).cpu().numpy().tolist()\n",
        "\n",
        "            # 結果を返す．\n",
        "            return result[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXvQbK9gisG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "ba428c71-8a22-4298-b3f1-09c2c7251f71"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "#文章をいくつ推論するか\n",
        "RETURN_NUM = 8\n",
        "#候補文をいくつ表示するか(RETURN_NUMより小さい値にしてください)\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "#BERTを使った評価用クラスの準備\n",
        "be = BertEvaluator()\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"retry\" == u:\n",
        "    next_sentence = log.pop()\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=RETURN_NUM)\n",
        "\n",
        "  sequence_dict = {}\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    score = be.evaluate(\"\".join(log[-1]), sequence[len(next_sentence):])\n",
        "    sequence_dict[sequence] = score\n",
        "  sequence_dict = sorted(sequence_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "  for i in range(OPTION_NUM):\n",
        "    print(\"[\", i,\"]\", sequence_dict[i][0])\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_dict[int(u)][0]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"retry\"と入力すると同じ文で再度文を生成します\n",
        "  # 7. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">今日はいい天気だ。\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] 今日はいい天気だ。 日が沈むのもそろそろだろう。 「今はもう日の沈むのも早くなったね。 「お前が夕焼けを見たいっていい出すんだから、それで日没が早いって言うんだろう」 そういって彼は私の方を向いて向うを向いてくれた。\n",
            "[ 1 ] 今日はいい天気だ。 こんな日でいいのかと自分で思ってしまう。 「もうすぐ雨になるんですか」 私は突然そんな風に尋ねた。\n",
            "[ 2 ] 今日はいい天気だ。 外が少し明るい。 窓から差し込む光は明るいように思われるけれども、外を見る限りそれは全く感じられない。 私は何だか変な風を切望していた。\n",
            "[ 3 ] 今日はいい天気だ。 今から散歩をしよう。 今日はどこへ行こうかな。 あそこへでも。\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a68d9fe80002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdecimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mchoice_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}